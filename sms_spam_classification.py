# -*- coding: utf-8 -*-
"""SMS_spam_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cqTXDhPd_Bgc0vxXoByijymuIHsrqss5
"""

# !pip install kaggle
# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
# !kaggle datasets download -d uciml/sms-spam-collection-dataset
# !unzip -q sms-spam-collection-dataset.zip

import pandas as pd
df = pd.read_csv("/content/spam.csv", encoding='latin-1')
df

df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)

df.rename(columns={'v1': 'target', 'v2': 'text'}, inplace=True)
df.head()

df['target'].value_counts().plot(kind='bar')
print(df['target'].value_counts())

import pandas as pd
from sklearn.utils import resample


df_majority = df[df['target'] == 'ham']
df_minority = df[df['target'] == 'spam']

df_majority_downsampled = resample(df_majority, replace=False,  n_samples=len(df_minority), random_state=123)

df_balanced = pd.concat([df_majority_downsampled, df_minority])
df=df_balanced

df['target'].value_counts().plot(kind='bar')

import re
import numpy as np

def remove_link(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    if pattern:
        return pattern.sub(r'', text)
    else:
        return text

import string
Exclude=string.punctuation



def remove_punctuation(text):
    return text.translate(str.maketrans('','',Exclude))

def remove_html_tag(text):
    i=0
    pattern=re.compile(r'<.*?>')
    if pattern:
      list=np.array(pattern.findall(text))
      n=list.shape[0]
    for i in range(n):
      text= text.replace(list[i],'')
    return text


from textblob import TextBlob

def correct_spell(text):
    textblb=TextBlob(text)
    return textblb.correct().string


import nltk

from nltk.corpus import stopwords

nltk.download('stopwords')


extra=stopwords.words('english')
stopwrd=np.array(extra)

def remove_stopwords(text, stopwords):
    split_text = text.split()
    filtered_text = []

    for word in split_text:
        if word not in stopwords:
            filtered_text.append(word)

    result = ' '.join(filtered_text)
    return result

def remove_emoji(text):
    pattern=re.compile('[\U00010000-\U0010ffff]', flags=re.UNICODE)
    if pattern:
       list= pattern.findall(text)
    return pattern.sub(r'', text)

import spacy
nlp = spacy.load("en_core_web_sm")

def lemmatization(text):
    doc=nlp(text)
    text_=[]
    for word in doc:
       text_.append(word.lemma_)

    result = ' '.join(text_)
    return result

def text_cleaning(text,stopwords):
    text1=remove_link(text)
    text2=remove_html_tag(text1)
    text3=remove_emoji(text2)
    text4=remove_stopwords(text3,stopwords)
    text5=remove_punctuation(text4)
    text6=lemmatization(text5)
    text7=correct_spell(text6)
    return text7

df['text']=df['text'].apply(text_cleaning,stopwords=stopwrd)

"""#Logistic_regression

"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.under_sampling import RandomUnderSampler

X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)

def ohe(text):

    if text=='ham':
         return 1
    return 0

y_train=y_train.apply(lambda x:ohe(x)).astype('int')
y_test=y_test.apply(lambda x:ohe(x)).astype('int')


tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)


log_reg = LogisticRegression()
log_reg.fit(X_train_tfidf, y_train)


y_pred = log_reg.predict(X_test_tfidf)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

df_new = df.copy()
df_new = df_new[df_new['text'].str.split().str.len() > 2]
df_new['text']=df_new['text'].apply(lambda x: '<start> '+ x + ' <end>')

print('maximum word count in a sentence',max(df_new['text'].str.split().str.len()))
print('minimum word count in a sentence',min(df_new['text'].str.split().str.len()))
print('median word count in a sentence',np.median(df_new['text'].str.split().str.len()))

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(df_new['text'],df_new['target'],test_size=0.01,random_state=2)

def ohe(text):

    if text=='ham':
         return 1
    return 0

Y_train=Y_train.apply(lambda x:ohe(x)).astype('int')

x_train_list = []

def create_list(text):
    x_train_list.append(text)

X_train.apply(lambda x: create_list(x))

print(x_train_list)

import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences

def tokenize_pad(list,max_len,num_words):
    tokenizer=Tokenizer(num_words=num_words)
    tokenizer.fit_on_texts(list)
    sequence=tokenizer.texts_to_sequences(list)
    padded=pad_sequences(sequence,padding='post',maxlen=max_len)
    return padded

total_claims_word=df_new['text'].str.split().str.len().sum()
max_len=max(df_new['text'].str.split().str.len())
print(total_claims_word)
x_input=tokenize_pad(x_train_list,max_len,total_claims_word)
x_input

from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D, Attention
from keras.models import Model
from keras.optimizers import Nadam
from keras.callbacks import ModelCheckpoint
import tensorflow as tf
import matplotlib.pyplot as plt

max_sequence_length=max_len


input_layer = Input(shape=(max_sequence_length,), name='claims_input')


embedding_layer = Embedding(total_claims_word,512, name='Embedding_layer')(input_layer)
x = BatchNormalization(name='Normalization_layer')(embedding_layer)


conv_layer = Conv1D(filters=256, kernel_size=5, activation='relu', name='Conv1D_layer')(x)
pooling_layer = MaxPooling1D(pool_size=2, name='MaxPooling_layer')(conv_layer)


bilstm_layer = Bidirectional(LSTM(100, return_sequences=True, name='BiLSTM_layer'))(pooling_layer)


attention = Attention(name='Attention_layer')([bilstm_layer, bilstm_layer])
attention_pooling = GlobalMaxPooling1D(name='Attention_Pooling_layer')(attention)


dense_layer = Dense(100, activation='relu', name='Dense_layer_1')(attention_pooling)
dropout_layer = Dropout(0.6, name='Dropout_layer_1')(dense_layer)
dense_layer_2 = Dense(50, activation='relu', name='Dense_layer_2')(dropout_layer)
dropout_layer_2 = Dropout(0.5, name='Dropout_layer_2')(dense_layer_2)


output_layer = Dense(2, activation='softmax', name='output_layer')(dropout_layer_2)



model = Model(inputs=input_layer, outputs=output_layer)
loss= tf.keras.losses.SparseCategoricalCrossentropy()
model.compile(optimizer=Nadam(learning_rate=0.001), loss=loss, metrics=['accuracy'])



model.summary()

checkpoint_callback = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)



batch_size = 1000
epochs = 100

history = model.fit(
    x_input,
    Y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.12,
    callbacks=[checkpoint_callback]
)

#graph
plt.plot(history.history['accuracy'], color='red',label='accuracy')
plt.plot(history.history['val_accuracy'],color='blue',label='val_accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

plt.plot(history.history['loss'], color='red',label='loss')
plt.plot(history.history['val_loss'],color='blue',label='val_loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

"""#Inference Class"""

class InferenceModel:
    def __init__(self, model_path='/content/best_model.h5'):
        self.model_path = model_path
        self.model = self.load_model()

    def load_model(self):
        return tf.keras.models.load_model(self.model_path)

    def predict(self, input_data):
        input_data = text_cleaning(input_data,extra)
        input_data = '<start> ' + input_data + ' <end>'
        list=[]
        list.append(input_data)
        input_data=tokenize_pad(list,max_len,total_claims_word)
        predictions = self.model.predict(input_data)
        return predictions

inference_model = InferenceModel()
def generate_output(text):
    text=text.lower()
    output = inference_model.predict(text)
    return output


probabilities = generate_output('''Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine
there got amore wat...''')

class_0_prob = probabilities[0, 0]
class_1_prob = probabilities[0, 1]

if class_0_prob > class_1_prob:
    class_label = 'spam'
else:
    class_label = 'ham'

classes = ['spam', 'ham']


values = [class_0_prob, class_1_prob]


colors = ['red','blue']


plt.figure(figsize=(8, 6))
plt.pie(values, labels=classes, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True, explode=(0.1, 0.0))


plt.title('Class Probabilities')


plt.show()

print("Class Label:", class_label)
print("Class 0 Probability:", class_0_prob)
print("Class 1 Probability:", class_1_prob)







